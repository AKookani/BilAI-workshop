{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from typing import *\n",
        "from glob import glob\n",
        "import gzip\n",
        "import re\n",
        "import numpy as np\n",
        "import random\n",
        "import csv\n",
        "import json\n",
        "from itertools import chain\n",
        "from collections import defaultdict\n",
        "import gc\n",
        "import os\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from http import HTTPStatus"
      ],
      "metadata": {
        "id": "9OG6R0fcxaKs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we load the constraints data\n",
        "constraints_def = {}\n",
        "\n",
        "mapping_to_wikidata = {\n",
        "    'property id': '^<http://www.wikidata.org/entity/P2302>',\n",
        "    ' constraint type id': '<http://www.wikidata.org/entity/P2302>',\n",
        "    'regex': '<http://www.wikidata.org/entity/P1793>',\n",
        "    'exceptions': '<http://www.wikidata.org/entity/P2303>',\n",
        "    'group by': '<http://www.wikidata.org/entity/P2304>',\n",
        "    'items': '<http://www.wikidata.org/entity/P2305>',\n",
        "    'property': '<http://www.wikidata.org/entity/P2306>',\n",
        "    'namespace': '<http://www.wikidata.org/entity/P2307>',\n",
        "    'class': '<http://www.wikidata.org/entity/P2308>',\n",
        "    'relation': '<http://www.wikidata.org/entity/P2309>',\n",
        "    'minimal date': '<http://www.wikidata.org/entity/P2310>',\n",
        "    'maximum date': '<http://www.wikidata.org/entity/P2311>',\n",
        "    'maximum value': '<http://www.wikidata.org/entity/P2312>',\n",
        "    'minimal value': '<http://www.wikidata.org/entity/P2313>',\n",
        "    'status': '<http://www.wikidata.org/entity/P2316>',\n",
        "    'separator': '<http://www.wikidata.org/entity/P4155>',\n",
        "    'scope': '<http://www.wikidata.org/entity/P4680>'\n",
        "}\n",
        "with open('/content/constraints.tsv', newline='') as fp:\n",
        "    for row in csv.DictReader(fp, dialect='excel-tab'):\n",
        "        predicates = []\n",
        "        objects = []\n",
        "        for k,vs in row.items():\n",
        "            if k != 'constraint id':\n",
        "                for v in vs.split(' '):\n",
        "                    v = v.strip()\n",
        "                    if v:\n",
        "                        predicates.append(mapping_to_wikidata[k])\n",
        "                        objects.append(v)\n",
        "        constraints_def[row['constraint id']] = {'predicates': predicates, 'objects': objects}"
      ],
      "metadata": {
        "id": "6qN5uswNIU8R"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we load and preprocess the data. We encode string with integers to allow the data to fit easily into the main memory\n",
        "\n",
        "class GlobalIntEncoder:\n",
        "    def __init__(self):\n",
        "        self._encoding = {\n",
        "            '': 0\n",
        "        }\n",
        "\n",
        "    def encode(self, value: str):\n",
        "        if value is None:\n",
        "            value = ''\n",
        "        value = str(value)\n",
        "        if value not in self._encoding:\n",
        "            self._encoding[value] = len(self._encoding)\n",
        "        return self._encoding[value]\n",
        "\n",
        "    def save(self, file: str):\n",
        "        with open(file, 'wt') as fp:\n",
        "            fp.writelines(l + '\\n' for l in self._encoding.keys())\n",
        "\n",
        "encoder = GlobalIntEncoder()\n",
        "\n",
        "_relation_to_predicate = {\n",
        "    encoder.encode('<http://www.wikidata.org/entity/Q21503252>'): [encoder.encode('<http://www.wikidata.org/entity/P31>')],\n",
        "    encoder.encode('<http://www.wikidata.org/entity/Q21514624>'): [encoder.encode('<http://www.wikidata.org/entity/P279>')],\n",
        "    encoder.encode('<http://www.wikidata.org/entity/Q30208840>'): [encoder.encode('<http://www.wikidata.org/entity/P31>'), encoder.encode('<http://www.wikidata.org/entity/P279>')],\n",
        "}\n",
        "\n",
        "def _convert_values(values: str) -> List[str]:\n",
        "    return [v for v in (_convert_value(v.strip()) for v in values.split(' ')) if v]\n",
        "\n",
        "def _convert_value(value: Optional[str], subject: Optional[str] = None, predicate: Optional[str] = None, obj: Optional[str] = None, other_subject: Optional[str] = None, other_predicate: Optional[str] = None, other_object: Optional[str] = None) -> Optional[str]:\n",
        "    if value is None or value == '':\n",
        "        return 0\n",
        "    value = encoder.encode(value.replace('http://www.wikidata.org/prop/direct/', 'http://www.wikidata.org/entity/'))\n",
        "    if value == subject:\n",
        "        return encoder.encode('subject')\n",
        "    elif value == predicate:\n",
        "        return encoder.encode('predicate')\n",
        "    elif value == obj:\n",
        "        return encoder.encode('object')\n",
        "    elif value == other_subject:\n",
        "        return encoder.encode('other_subject')\n",
        "    elif value == other_predicate:\n",
        "        return encoder.encode('other_predicate')\n",
        "    elif value == other_object:\n",
        "        return encoder.encode('other_object')\n",
        "    else:\n",
        "        return value\n",
        "\n",
        "def _read_entity_desc(line: List[str], desc_position: int) -> Dict[str,Any]:\n",
        "    desc = line[desc_position].strip()\n",
        "    result = {\n",
        "            'entity_predicates': [],\n",
        "            'entity_objects': [],\n",
        "            'entity_labels': [],\n",
        "            'http_content': ''\n",
        "    }\n",
        "    if not desc:\n",
        "        return result\n",
        "    try:\n",
        "        desc = json.loads(desc)\n",
        "    except ValueError:\n",
        "        print('Invalid description: {}'.format(desc))\n",
        "        return result\n",
        "    if desc['type'] == 'page':\n",
        "        try:\n",
        "            status = \"<http://www.w3.org/2011/http-statusCodes#{}>\".format(HTTPStatus(desc['statusCode']).phrase.title().replace(' ', '').replace('-', ''))\n",
        "            result['entity_predicates'].append(_convert_value('<http://wikiba.se/history/ontology#pageStatusCode>'))\n",
        "            result['entity_objects'].append(_convert_value(status))\n",
        "        except ValueError as e:\n",
        "            print(e)\n",
        "        result['http_content'] = desc['content']\n",
        "    elif desc['type'] == 'entity':\n",
        "        result['entity_labels'].extend(desc['labels'].values())\n",
        "        for predicate, objects in desc['facts'].items():\n",
        "            for obj in objects:\n",
        "                result['entity_predicates'].append(_convert_value(predicate))\n",
        "                result['entity_objects'].append(_convert_value(obj))\n",
        "    else:\n",
        "        print('Invalid description: {}'.format(result))\n",
        "    return result\n",
        "\n",
        "def load_dataset(file_path, max_size: int = 100000):\n",
        "    dataset = {\n",
        "        'constraint_id': [],\n",
        "        'constraint_predicates': [],\n",
        "        'constraint_objects': [],\n",
        "        'subject': [],\n",
        "        'predicate': [],\n",
        "        'object': [],\n",
        "        'object_text': [],\n",
        "        'other_subject': [],\n",
        "        'other_predicate': [],\n",
        "        'other_object': [],\n",
        "        'other_object_text': [],\n",
        "        'subject_predicates': [],\n",
        "        'subject_objects': [],\n",
        "        'object_predicates': [],\n",
        "        'object_objects': [],\n",
        "        'other_entity_predicates': [],\n",
        "        'other_entity_objects': [],\n",
        "        'add_subject': [],\n",
        "        'add_predicate': [],\n",
        "        'add_object': [],\n",
        "        'del_subject': [],\n",
        "        'del_predicate': [],\n",
        "        'del_object': []\n",
        "    }\n",
        "    with gzip.open(file_path, 'rt') as fp:\n",
        "        for line_i, line in enumerate(fp):\n",
        "            if line_i == max_size:\n",
        "                break\n",
        "\n",
        "            elements = line.split('\\t')\n",
        "            if elements[0] not in constraints_def:\n",
        "                continue\n",
        "\n",
        "            constraint = constraints_def[elements[0]]\n",
        "            subject = _convert_value(elements[2])\n",
        "            predicate = _convert_value(elements[3])\n",
        "            obj = _convert_value(elements[4])\n",
        "            other_subject = _convert_value(elements[5])\n",
        "            other_predicate = _convert_value(elements[6])\n",
        "            other_object = _convert_value(elements[7])\n",
        "            add_subject = None\n",
        "            add_predicate = None\n",
        "            add_object = None\n",
        "            del_subject= None\n",
        "            del_predicate = None\n",
        "            del_object = None\n",
        "            entity = None\n",
        "            i = 12\n",
        "            while i < len(elements):\n",
        "                if elements[i] == '<http://wikiba.se/history/ontology#addition>':\n",
        "                    add_subject = elements[i - 3]\n",
        "                    add_predicate = elements[i - 2]\n",
        "                    add_object = elements[i - 1]\n",
        "                elif elements[i] == '<http://wikiba.se/history/ontology#deletion>':\n",
        "                    del_subject = elements[i - 3]\n",
        "                    del_predicate = elements[i - 2]\n",
        "                    del_object = elements[i - 1]\n",
        "                else:\n",
        "                    print('Unexpected entity: {}'.format(elements[i-3:i+1]))\n",
        "                    continue\n",
        "                i += 4\n",
        "\n",
        "            subject_desc = _read_entity_desc(elements, -3)\n",
        "            object_desc = _read_entity_desc(elements, -2)\n",
        "            other_entity_desc = _read_entity_desc(elements, -1)\n",
        "            if any(label in object_desc['http_content'] for label in subject_desc['entity_labels']):\n",
        "                object_desc['entity_predicates'].append(_convert_value('<http://wikiba.se/history/ontology#pageContainsLabel>'))\n",
        "                object_desc['entity_objects'].append(subject)\n",
        "            if any(label in object_desc['http_content'] for label in other_entity_desc['entity_labels']):\n",
        "                object_desc['entity_predicates'].append(_convert_value('<http://wikiba.se/history/ontology#pageContainsLabel>'))\n",
        "                object_desc['entity_objects'].append(other_subject)\n",
        "            if any(label in other_entity_desc['http_content'] for label in subject_desc['entity_labels']):\n",
        "                other_entity_desc['entity_predicates'].append(_convert_value('<http://wikiba.se/history/ontology#pageContainsLabel>'))\n",
        "                other_entity_desc['entity_objects'].append(subject)\n",
        "            if any(label in other_entity_desc['http_content'] for label in object_desc['entity_labels']):\n",
        "                other_entity_desc['entity_predicates'].append(_convert_value('<http://wikiba.se/history/ontology#pageContainsLabel>'))\n",
        "                other_entity_desc['entity_objects'].append(obj)\n",
        "\n",
        "            dataset['constraint_id'].append(_convert_value(elements[0]))\n",
        "            dataset['constraint_predicates'].append([_convert_value(v) for v in constraint['predicates']])\n",
        "            dataset['constraint_objects'].append([_convert_value(v) for v in constraint['objects']])\n",
        "            dataset['subject'].append(subject)\n",
        "            dataset['predicate'].append(predicate)\n",
        "            if elements[4].startswith('<http://www.wikidata.org/entity/'):\n",
        "                dataset['object'].append(obj)\n",
        "                dataset['object_text'].append('')\n",
        "            else:\n",
        "                dataset['object'].append(0)\n",
        "                dataset['object_text'].append(elements[4].split('^^')[0])\n",
        "            dataset['other_subject'].append(other_subject)\n",
        "            dataset['other_predicate'].append(other_predicate)\n",
        "            if elements[7].startswith('<http://www.wikidata.org/entity/'):\n",
        "                dataset['other_object'].append(other_object)\n",
        "                dataset['other_object_text'].append('')\n",
        "            else:\n",
        "                dataset['other_object'].append(0)\n",
        "                dataset['other_object_text'].append(elements[7].split('^^')[0])\n",
        "            dataset['add_subject'].append(_convert_value(add_subject, subject, predicate, obj, other_subject, other_predicate, other_object))\n",
        "            dataset['add_predicate'].append(_convert_value(add_predicate, subject, predicate, obj, other_subject, other_predicate, other_object))\n",
        "            dataset['add_object'].append(_convert_value(add_object, subject, predicate, obj, other_subject, other_predicate, other_object))\n",
        "            dataset['del_subject'].append(_convert_value(del_subject, subject, predicate, obj, other_subject, other_predicate, other_object))\n",
        "            dataset['del_predicate'].append(_convert_value(del_predicate, subject, predicate, obj, other_subject, other_predicate, other_object))\n",
        "            dataset['del_object'].append(_convert_value(del_object, subject, predicate, obj, other_subject, other_predicate, other_object))\n",
        "            dataset['subject_predicates'].append(subject_desc['entity_predicates'])\n",
        "            dataset['subject_objects'].append(subject_desc['entity_objects'])\n",
        "            dataset['object_predicates'].append(object_desc['entity_predicates'])\n",
        "            dataset['object_objects'].append(object_desc['entity_objects'])\n",
        "            dataset['other_entity_predicates'].append(other_entity_desc['entity_predicates'])\n",
        "            dataset['other_entity_objects'].append(other_entity_desc['entity_objects'])\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "v-k_fSm7IZjC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# 1. Unzip the dataset\n",
        "with zipfile.ZipFile('/content/constraint-corrections.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/constraint-corrections')  # Extracts to /content/constraint-corrections/\n",
        "\n",
        "# 2. Verify extraction (optional)\n",
        "print(\"Extracted files:\", os.listdir('/content/constraint-corrections'))\n",
        "\n",
        "# 3. Modify your `load` function to use the correct path\n",
        "def load(kind: str, targets: List[str]):\n",
        "    result = defaultdict(list)\n",
        "    for target in targets:\n",
        "        print(f'Loading {target} {kind}')\n",
        "        file_path = f'/content/constraint-corrections/constraint-corrections-{target}.tsv.gz.full.{kind}.tsv.gz'\n",
        "        data = load_dataset(file_path)\n",
        "        for k, v in data.items():\n",
        "            result[k].extend(v)  # Keep as lists\n",
        "    gc.collect()\n",
        "    return result\n",
        "\n",
        "# Now run your original code\n",
        "target = '*'\n",
        "if target == '*':\n",
        "    targets = ['conflictWith', 'distinct', 'inverse', 'itemRequiresStatement', 'oneOf', 'single', 'type', 'valueRequiresStatement', 'valueType']\n",
        "    train_dataset = load('train', targets)\n",
        "    dev_dataset = load('dev', targets)\n",
        "    test_dataset = {target: load('test', [target]) for target in targets}\n",
        "else:\n",
        "    train_dataset = load('train', [target])\n",
        "    dev_dataset = load('dev', [target])\n",
        "    test_dataset = {target: load('test', [target])}\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtJFKXoSJsNK",
        "outputId": "eeff1c47-74c5-496f-d5e3-d6c70e4098f7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted files: ['constraint-corrections-valueRequiresStatement.tsv.gz.full.test.tsv.gz', 'constraint-corrections-distinct.tsv.gz.full.train.tsv.gz', 'constraint-corrections-oneOf.tsv.gz.full.train.tsv.gz', 'constraint-corrections-type.tsv.gz.full.train.tsv.gz', 'constraint-corrections-valueType.tsv.gz.full.dev.tsv.gz', 'constraint-corrections-distinct.tsv.gz.full.dev.tsv.gz', 'constraint-corrections-single.tsv.gz.full.test.tsv.gz', 'constraint-corrections-valueType.tsv.gz.full.train.tsv.gz', 'constraint-corrections-type.tsv.gz.full.test.tsv.gz', 'constraint-corrections-valueRequiresStatement.tsv.gz.full.train.tsv.gz', 'constraint-corrections-single.tsv.gz.full.dev.tsv.gz', 'constraint-corrections-oneOf.tsv.gz.full.dev.tsv.gz', 'constraint-corrections-conflictWith.tsv.gz.full.test.tsv.gz', 'constraint-corrections-itemRequiresStatement.tsv.gz.full.test.tsv.gz', 'constraint-corrections-inverse.tsv.gz.full.test.tsv.gz', 'constraint-corrections-valueType.tsv.gz.full.test.tsv.gz', 'constraint-corrections-conflictWith.tsv.gz.full.dev.tsv.gz', 'constraint-corrections-inverse.tsv.gz.full.dev.tsv.gz', 'constraint-corrections-single.tsv.gz.full.train.tsv.gz', 'constraint-corrections-inverse.tsv.gz.full.train.tsv.gz', 'constraint-corrections-oneOf.tsv.gz.full.test.tsv.gz', 'constraint-corrections-distinct.tsv.gz.full.test.tsv.gz', 'constraint-corrections-valueRequiresStatement.tsv.gz.full.dev.tsv.gz', 'constraint-corrections-itemRequiresStatement.tsv.gz.full.dev.tsv.gz', 'constraint-corrections-itemRequiresStatement.tsv.gz.full.train.tsv.gz', 'constraint-corrections-type.tsv.gz.full.dev.tsv.gz', 'constraint-corrections-conflictWith.tsv.gz.full.train.tsv.gz']\n",
            "Loading conflictWith train\n",
            "Loading distinct train\n",
            "Loading inverse train\n",
            "Loading itemRequiresStatement train\n",
            "Loading oneOf train\n",
            "Loading single train\n",
            "Loading type train\n",
            "Loading valueRequiresStatement train\n",
            "Loading valueType train\n",
            "Loading conflictWith dev\n",
            "Loading distinct dev\n",
            "Loading inverse dev\n",
            "Loading itemRequiresStatement dev\n",
            "Loading oneOf dev\n",
            "Loading single dev\n",
            "Loading type dev\n",
            "Loading valueRequiresStatement dev\n",
            "Loading valueType dev\n",
            "Loading conflictWith test\n",
            "Loading distinct test\n",
            "Loading inverse test\n",
            "Loading itemRequiresStatement test\n",
            "Loading oneOf test\n",
            "Loading single test\n",
            "Loading type test\n",
            "Loading valueRequiresStatement test\n",
            "Loading valueType test\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all dataset values to numpy arrays, handling variable-length sequences\n",
        "def convert_to_numpy(data):\n",
        "    for key in data.keys():\n",
        "        # For lists of numbers, convert directly\n",
        "        if all(isinstance(x, (int, float)) for x in data[key]):\n",
        "            data[key] = np.array(data[key])\n",
        "        # For lists of lists (variable length sequences), convert to numpy object array\n",
        "        else:\n",
        "            data[key] = np.array(data[key], dtype=object)\n",
        "    return data\n",
        "\n",
        "# Apply conversion to all datasets\n",
        "train_dataset = convert_to_numpy(train_dataset)\n",
        "dev_dataset = convert_to_numpy(dev_dataset)\n",
        "test_dataset = {target: convert_to_numpy(test_dataset[target]) for target in test_dataset}"
      ],
      "metadata": {
        "id": "nzoO7Jrnib9v"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prints some statistics about the dataset\n",
        "from collections import defaultdict\n",
        "\n",
        "def dataset_stats(dataset: Iterable[dict]):\n",
        "    known_entities = [encoder.encode('subject'), encoder.encode('predicate'), encoder.encode('object'), encoder.encode('other_subject'), encoder.encode('other_predicate'), encoder.encode('other_object'), encoder.encode('constraint_predicate')]\n",
        "    count = 0\n",
        "    constraints = defaultdict(int)\n",
        "    with_subject_desc = 0\n",
        "    subject_desc_sum = 0\n",
        "    with_object_desc = 0\n",
        "    object_desc_sum = 0\n",
        "    with_object_http_status = 0\n",
        "    object_http_contains_label = 0\n",
        "    with_other_triple = 0\n",
        "    with_other_entity_desc = 0\n",
        "    other_entity_desc_sum = 0\n",
        "    with_other_entity_http_status = 0\n",
        "    other_entity_http_contains_label = 0\n",
        "    with_add_subject = 0\n",
        "    with_add_predicate = 0\n",
        "    with_add_object = 0\n",
        "    with_del_subject = 0\n",
        "    with_del_predicate = 0\n",
        "    with_del_object = 0\n",
        "    with_add_subject_in_input = 0\n",
        "    with_add_predicate_in_input = 0\n",
        "    with_add_object_in_input = 0\n",
        "    with_del_subject_in_input = 0\n",
        "    with_del_predicate_in_input = 0\n",
        "    with_del_object_in_input = 0\n",
        "    for i in range(len(dataset['predicate'])):\n",
        "        count += 1\n",
        "        constraints[dataset['constraint_id'][i]] += 1\n",
        "        if dataset['subject_predicates'][i]:\n",
        "            with_subject_desc += 1\n",
        "            subject_desc_sum += len(dataset['subject_predicates'][i])\n",
        "            assert len(dataset['subject_predicates'][i]) == len(dataset['subject_objects'][i]) # and len(dataset['subject_predicates'][i]) == len(dataset['subject_objects_text'][i])\n",
        "        if dataset['object_predicates'][i]:\n",
        "            with_object_desc += 1\n",
        "            object_desc_sum += len(dataset['object_predicates'][i])\n",
        "            assert len(dataset['object_predicates'][i]) == len(dataset['object_objects'][i]) # and len(dataset['object_predicates'][i]) == len(dataset['object_objects_text'][i])\n",
        "        if encoder.encode('<http://wikiba.se/history/ontology#pageStatusCode>') in dataset['object_predicates'][i]:\n",
        "            with_object_http_status += 1\n",
        "        if encoder.encode('<http://wikiba.se/history/ontology#pageContainsLabel>') in dataset['object_predicates'][i]:\n",
        "            object_http_contains_label += 1\n",
        "        if dataset['other_subject'][i]:\n",
        "            with_other_triple += 1\n",
        "        if dataset['other_entity_predicates'][i]:\n",
        "            with_other_entity_desc += 1\n",
        "            other_entity_desc_sum += len(dataset['other_entity_predicates'][i])\n",
        "            assert len(dataset['other_entity_predicates'][i]) == len(dataset['other_entity_objects'][i]) # and len(dataset['other_entity_predicates'][i]) == len(dataset['other_entity_objects_text'][i])\n",
        "        if encoder.encode('<http://wikiba.se/history/ontology#pageStatusCode>') in dataset['other_entity_predicates'][i]:\n",
        "            with_other_entity_http_status += 1\n",
        "        if encoder.encode('<http://wikiba.se/history/ontology#pageContainsLabel>') in dataset['other_entity_predicates'][i]:\n",
        "            other_entity_http_contains_label += 1\n",
        "        if dataset['add_subject'][i]:\n",
        "            with_add_subject += 1\n",
        "            if dataset['add_subject'][i] in known_entities:\n",
        "                with_add_subject_in_input += 1\n",
        "        if dataset['add_predicate'][i]:\n",
        "            with_add_predicate += 1\n",
        "            if dataset['add_predicate'][i] in known_entities:\n",
        "                with_add_predicate_in_input += 1\n",
        "        if dataset['add_object'][i]:\n",
        "            with_add_object += 1\n",
        "            if dataset['add_object'][i] in known_entities:\n",
        "                with_add_object_in_input += 1\n",
        "        if dataset['del_subject'][i]:\n",
        "            with_del_subject += 1\n",
        "            if dataset['del_subject'][i] in known_entities:\n",
        "                with_del_subject_in_input += 1\n",
        "        if dataset['del_predicate'][i]:\n",
        "            with_del_predicate += 1\n",
        "            if dataset['del_predicate'][i] in known_entities:\n",
        "                with_del_predicate_in_input += 1\n",
        "        if dataset['del_object'][i]:\n",
        "            with_del_object += 1\n",
        "            if dataset['del_object'][i] in known_entities:\n",
        "                with_del_object_in_input += 1\n",
        "    print('{} past violations for {} constraints'.format(sum(constraints.values()), len(constraints)))\n",
        "    print('with subject desc: {} (average length: {})'.format(with_subject_desc / count, subject_desc_sum / with_subject_desc))\n",
        "    print('with object desc: {} (average length: {})'.format(with_object_desc / count, object_desc_sum / with_object_desc))\n",
        "    print('with object web page: {} (with label in page: {})'.format(with_object_http_status / count, object_http_contains_label / with_object_http_status if with_object_http_status else '?'))\n",
        "    print('with other triple: {} ({})'.format(with_other_triple, with_other_triple / count))\n",
        "    print('with other entity desc: {} (average length: {})'.format(with_other_entity_desc / count, other_entity_desc_sum / with_other_entity_desc if with_other_entity_desc else '?'))\n",
        "    print('with other entity web page: {} (with label in page: {})'.format(with_other_entity_http_status / count, other_entity_http_contains_label / with_other_entity_http_status if with_other_entity_http_status else '?'))\n",
        "    print('in input: add subject: {} add predicate: {} add object: {} del subject: {} del predicate: {} del object: {}'.format(with_add_subject_in_input / with_add_subject, with_add_predicate_in_input / with_add_predicate, with_add_object_in_input / with_add_object, with_del_subject_in_input / with_del_subject, with_del_predicate_in_input / with_del_predicate, with_del_object_in_input / with_del_object))\n",
        "    print('add: {} ({}, subject {} known object {} known)'.format(with_add_subject, with_add_subject / count, with_add_subject_in_input / with_add_subject if with_add_subject else '?', with_add_object_in_input / with_add_object if with_add_object else '?'))\n",
        "    print('del: {} ({}, subject {} known object {} known)'.format(with_del_subject, with_del_subject / count, with_del_subject_in_input / with_del_subject if with_del_subject else '?', with_del_object_in_input / with_del_object if with_del_object else '?'))\n",
        "dataset_stats(dev_dataset)"
      ],
      "metadata": {
        "id": "9qEOOj_EJyHm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe95210d-4f65-4784-d061-aad89cc150dd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4500 past violations for 649 constraints\n",
            "with subject desc: 0.992 (average length: 16.012096774193548)\n",
            "with object desc: 0.798 (average length: 19.10359231411863)\n",
            "with object web page: 0.1648888888888889 (with label in page: 0.5646900269541779)\n",
            "with other triple: 1449 (0.322)\n",
            "with other entity desc: 0.2768888888888889 (average length: 7.06099518459069)\n",
            "with other entity web page: 0.08355555555555555 (with label in page: 0.5452127659574468)\n",
            "in input: add subject: 1.0 add predicate: 0.278336686787391 add object: 0.18376928236083165 del subject: 1.0 del predicate: 1.0 del object: 0.9901869158878505\n",
            "add: 2982 (0.6626666666666666, subject 1.0 known object 0.18376928236083165 known)\n",
            "del: 2140 (0.47555555555555556, subject 1.0 known object 0.9901869158878505 known)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train the BERT tokenizer\n",
        "with open('raw_corrections_text.txt', 'wt') as fp:\n",
        "    for line in chain(\n",
        "        train_dataset['object_text'],\n",
        "        train_dataset['other_object_text']\n",
        "    ):\n",
        "        if line:\n",
        "            fp.write(line)\n",
        "            fp.write('\\n')\n",
        "tokenizer = BertWordPieceTokenizer(\n",
        "    clean_text=True,\n",
        "    handle_chinese_chars=True,\n",
        "    strip_accents=True,\n",
        "    lowercase=True,\n",
        "    pad_token=\"[PAD]\",\n",
        "    unk_token=\"[UNK]\",\n",
        "    sep_token=\"[SEP]\",\n",
        "    cls_token=\"[CLS]\",\n",
        "    mask_token=\"[MASK]\"\n",
        ")\n",
        "tokenizer.train('raw_corrections_text.txt', vocab_size=30000)\n",
        "os.remove('raw_corrections_text.txt')"
      ],
      "metadata": {
        "id": "ldYmJUthO_Se"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final preprocessing and model code\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "SEQUENCE_SIZE = 64\n",
        "tokenizer.enable_padding(length=SEQUENCE_SIZE, pad_token=\"[PAD]\")  # Note 'length' instead of 'max_length'\n",
        "tokenizer.enable_truncation(max_length=SEQUENCE_SIZE)\n",
        "\n",
        "\n",
        "def tokenize_sequence(sequence: List[str]) -> np.array:\n",
        "    matrix = np.zeros((len(sequence), SEQUENCE_SIZE), dtype='int32')\n",
        "    for i,v in enumerate(tokenizer.encode_batch(list(sequence))):\n",
        "        matrix[i] = v.ids\n",
        "    return matrix\n",
        "\n",
        "class TermEncoder:\n",
        "    def __init__(self,  min_occurences_count, max_sequence_length):\n",
        "        self._terms_index = {0: 0}\n",
        "        self._terms_inverse_index = [0]\n",
        "        self._terms_count = {}\n",
        "        self._min_occurences_count = min_occurences_count\n",
        "        self._max_sequence_length = max_sequence_length\n",
        "\n",
        "    def fit(self, input: str):\n",
        "        for value in input:\n",
        "            if value not in self._terms_index:\n",
        "                c = self._terms_count.get(value, 0) + 1\n",
        "                self._terms_count[value] = c\n",
        "                if c == self._min_occurences_count:\n",
        "                    self._terms_index[value] = len(self._terms_inverse_index)\n",
        "                    self._terms_inverse_index.append(value)\n",
        "                    del self._terms_count[value]\n",
        "\n",
        "    def fit_sequence(self, sequence: np.array):\n",
        "        for values in sequence:\n",
        "            self.fit(values)\n",
        "\n",
        "    def transform(self, input: np.array) -> np.array:\n",
        "        return np.array([self._terms_index.get(v, 0) for v in input])\n",
        "\n",
        "    def transform_sequence(self, input: np.array) -> np.array:\n",
        "        return keras.preprocessing.sequence.pad_sequences([[self._terms_index.get(v, 0) for v in values] for values in input], maxlen=self._max_sequence_length)\n",
        "\n",
        "    def decode(self, input: np.array) -> np.array:\n",
        "        return np.array([self._terms_inverse_index[v] for v in input])\n",
        "\n",
        "    def save(self, file: str):\n",
        "        with open(file, 'wt') as fp:\n",
        "            fp.writelines('{}\\n'.format(l) for l in self._terms_inverse_index)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._terms_inverse_index)\n",
        "\n",
        "class DatasetSequence(keras.utils.Sequence):\n",
        "    def __init__(self, dataset, constraint_id_encoder, predicate_encoder, entity_encoder, output_predicate_encoder, output_entity_encoder, batch_size: int, shuffle: bool = False):\n",
        "        self.dataset = dataset\n",
        "        self._constraint_id_encoder = constraint_id_encoder\n",
        "        self._entity_encoder = entity_encoder\n",
        "        self._predicate_encoder = predicate_encoder\n",
        "        self._output_entity_encoder = output_entity_encoder\n",
        "        self._output_predicate_encoder = output_predicate_encoder\n",
        "        self.batch_size = batch_size\n",
        "        self._shuffle = shuffle\n",
        "        self.indices = np.arange(len(self.dataset['add_subject']))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset['add_subject']) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inds = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        print(f\"Indices: {inds}\")  # Debug: Check indices\n",
        "        if not isinstance(inds, np.ndarray):\n",
        "            inds = np.array(inds)\n",
        "\n",
        "        # Get batch data\n",
        "        batch = {\n",
        "            'constraint_id': np.array(self.dataset['constraint_id'])[inds],\n",
        "            'constraint_predicates': np.array(self.dataset['constraint_predicates'])[inds],\n",
        "            'constraint_objects': np.array(self.dataset['constraint_objects'])[inds],\n",
        "            'subject': np.array(self.dataset['subject'])[inds],\n",
        "            'subject_predicates': np.array(self.dataset['subject_predicates'])[inds],\n",
        "            'subject_objects': np.array(self.dataset['subject_objects'])[inds],\n",
        "            'predicate': np.array(self.dataset['predicate'])[inds],\n",
        "            'object': np.array(self.dataset['object'])[inds],\n",
        "            'object_text': np.array(self.dataset['object_text'])[inds],\n",
        "            'object_predicates': np.array(self.dataset['object_predicates'])[inds],\n",
        "            'object_objects': np.array(self.dataset['object_objects'])[inds],\n",
        "            'other_subject': np.array(self.dataset['other_subject'])[inds],\n",
        "            'other_predicate': np.array(self.dataset['other_predicate'])[inds],\n",
        "            'other_object': np.array(self.dataset['other_object'])[inds],\n",
        "            'other_object_text': np.array(self.dataset['other_object_text'])[inds],\n",
        "            'other_entity_predicates': np.array(self.dataset['other_entity_predicates'])[inds],\n",
        "            'other_entity_objects': np.array(self.dataset['other_entity_objects'])[inds],\n",
        "            'add_subject': np.array(self.dataset['add_subject'])[inds],\n",
        "            'add_predicate': np.array(self.dataset['add_predicate'])[inds],\n",
        "            'add_object': np.array(self.dataset['add_object'])[inds],\n",
        "            'del_subject': np.array(self.dataset['del_subject'])[inds],\n",
        "            'del_predicate': np.array(self.dataset['del_predicate'])[inds],\n",
        "            'del_object': np.array(self.dataset['del_object'])[inds]\n",
        "        }\n",
        "\n",
        "        # Debug: Print batch keys and shapes\n",
        "        print(\"Batch keys:\", batch.keys())\n",
        "        for key, value in batch.items():\n",
        "            print(f\"{key}: shape={np.array(value).shape}\")\n",
        "\n",
        "        # Transform the batch data\n",
        "        x = {\n",
        "            'constraint_id': self._constraint_id_encoder.transform(batch['constraint_id']),\n",
        "            'constraint_predicates': self._predicate_encoder.transform_sequence(batch['constraint_predicates']),\n",
        "            'constraint_objects': self._predicate_encoder.transform_sequence(batch['constraint_objects']),\n",
        "            'subject': self._entity_encoder.transform(batch['subject']),\n",
        "            'subject_predicates': self._predicate_encoder.transform_sequence(batch['subject_predicates']),\n",
        "            'subject_objects': self._entity_encoder.transform_sequence(batch['subject_objects']),\n",
        "            'predicate': self._predicate_encoder.transform(batch['predicate']),\n",
        "            'object': self._entity_encoder.transform(batch['object']),\n",
        "            'object_text': tokenize_sequence(batch['object_text']),\n",
        "            'object_predicates': self._predicate_encoder.transform_sequence(batch['object_predicates']),\n",
        "            'object_objects': self._entity_encoder.transform_sequence(batch['object_objects']),\n",
        "            'other_subject': self._entity_encoder.transform(batch['other_subject']),\n",
        "            'other_predicate': self._predicate_encoder.transform(batch['other_predicate']),\n",
        "            'other_object': self._entity_encoder.transform(batch['other_object']),\n",
        "            'other_object_text': tokenize_sequence(batch['other_object_text']),\n",
        "            'other_entity_predicates': self._predicate_encoder.transform_sequence(batch['other_entity_predicates']),\n",
        "            'other_entity_objects': self._entity_encoder.transform_sequence(batch['other_entity_objects'])\n",
        "        }\n",
        "\n",
        "        y = {\n",
        "            'add_subject': self._output_entity_encoder.transform(batch['add_subject']),\n",
        "            'add_predicate': self._output_predicate_encoder.transform(batch['add_predicate']),\n",
        "            'add_object': self._output_entity_encoder.transform(batch['add_object']),\n",
        "            'del_subject': self._output_entity_encoder.transform(batch['del_subject']),\n",
        "            'del_predicate': self._output_predicate_encoder.transform(batch['del_predicate']),\n",
        "            'del_object': self._output_entity_encoder.transform(batch['del_object'])\n",
        "        }\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        gc.collect()\n",
        "        if self._shuffle:\n",
        "            print('shuffling dataset')\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "class EntityFactsEmbedding:\n",
        "    def __init__(self, entities_count: int, predicates_count: int, dropout: keras.layers.Dropout):\n",
        "        self._dropout = dropout\n",
        "        self._entity_predicates_embedding = keras.layers.Embedding(predicates_count, 128, mask_zero=True, name=\"predicate_embedding\")\n",
        "        self._entity_objects_embedding = keras.layers.Embedding(entities_count, 128, mask_zero=True, name=\"entity_embedding\")\n",
        "        self._entity_desc_combination = keras.layers.Concatenate(-1, name=\"entity_desc_combination\")\n",
        "        self._entity_desc_featurizer = keras.layers.Dense(128, activation='relu', name=\"entity_desc_featurizer\")\n",
        "        self._entity_desc_attention = tf.keras.layers.Attention(name=\"entity_desc_attention\")\n",
        "        self.to_sequence = keras.layers.Reshape((1,128), name=\"entity_desc_to_sequence\")\n",
        "        self._from_sequence = keras.layers.Reshape((128,), name=\"entity_desc_from_sequence\")\n",
        "        self._query_featurizer = keras.layers.Dense(128, activation='relu', name=\"entity_desc_query_featurizer\")\n",
        "        self._entity_desc_pool = keras.layers.GlobalMaxPooling1D(name=\"entity_desc_reduction\")\n",
        "\n",
        "    def __call__(self, predicates, objects, query = None):\n",
        "        value = self._entity_desc_featurizer(\n",
        "            self._dropout(\n",
        "                self._entity_desc_combination([\n",
        "                    self._entity_predicates_embedding(predicates),\n",
        "                    self._entity_objects_embedding(objects),\n",
        "                ])\n",
        "            )\n",
        "        )\n",
        "        if query is None:\n",
        "            return self._entity_desc_pool(value)\n",
        "        else:\n",
        "            return self._from_sequence(self._entity_desc_attention(\n",
        "                [self.to_sequence(self._query_featurizer(query)), value],\n",
        "            ))\n",
        "\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, train_dataset: Dict[str,np.array], dev_dataset: Dict[str,np.array], epochs: int = 2,  batch_size: int = 32, dropout: float = 0., with_attention: bool = False, with_entity_facts: bool = True, with_literals: bool = True, with_constraint_id: bool = False, with_subject: bool = False):\n",
        "        embedding_size = 128\n",
        "        min_occurences = 100\n",
        "        dropout_layer = keras.layers.Dropout(dropout, name=\"dropout\")\n",
        "\n",
        "        self._entity_encoder = TermEncoder(min_occurences, SEQUENCE_SIZE)\n",
        "        self._entity_encoder.fit_sequence(train_dataset['constraint_objects'])\n",
        "        self._entity_encoder.fit(train_dataset['subject'])\n",
        "        self._entity_encoder.fit(train_dataset['object'])\n",
        "        self._entity_encoder.fit(train_dataset['other_subject'])\n",
        "        self._entity_encoder.fit(train_dataset['other_object'])\n",
        "        self._entity_encoder.fit_sequence(train_dataset['subject_objects'])\n",
        "        self._entity_encoder.fit_sequence(train_dataset['object_objects'])\n",
        "        self._entity_encoder.fit_sequence(train_dataset['other_entity_objects'])\n",
        "\n",
        "        self._output_entity_encoder = TermEncoder(min_occurences, SEQUENCE_SIZE)\n",
        "        self._output_entity_encoder.fit(train_dataset['add_subject'])\n",
        "        self._output_entity_encoder.fit(train_dataset['add_object'])\n",
        "        self._output_entity_encoder.fit(train_dataset['del_subject'])\n",
        "        self._output_entity_encoder.fit(train_dataset['del_object'])\n",
        "\n",
        "        self._predicate_encoder = TermEncoder(min_occurences, SEQUENCE_SIZE)\n",
        "        self._predicate_encoder.fit_sequence(train_dataset['constraint_predicates'])\n",
        "        self._predicate_encoder.fit(train_dataset['predicate'])\n",
        "        self._predicate_encoder.fit(train_dataset['other_predicate'])\n",
        "        self._predicate_encoder.fit_sequence(train_dataset['subject_predicates'])\n",
        "        self._predicate_encoder.fit_sequence(train_dataset['object_predicates'])\n",
        "        self._predicate_encoder.fit_sequence(train_dataset['other_entity_predicates'])\n",
        "\n",
        "        self._output_predicate_encoder = TermEncoder(min_occurences, SEQUENCE_SIZE)\n",
        "        self._output_predicate_encoder.fit(train_dataset['add_predicate'])\n",
        "        self._output_predicate_encoder.fit(train_dataset['del_predicate'])\n",
        "\n",
        "        self._constraint_id_encoder = TermEncoder(min_occurences, SEQUENCE_SIZE)\n",
        "        self._constraint_id_encoder.fit(train_dataset['constraint_id'])\n",
        "\n",
        "        print('Dataset stats: {} input predicates, {} input entities, {} output predicates, {} output entities'.format(len(self._predicate_encoder), len(self._entity_encoder), len(self._output_predicate_encoder), len(self._output_entity_encoder)))\n",
        "\n",
        "        word_embedding = keras.layers.Embedding(30000, embedding_size, mask_zero=True, name=\"text_embedding\")\n",
        "        text_pool = keras.layers.GlobalMaxPool1D(name=\"text_pool\")\n",
        "        #text_pool = keras.layers.Bidirectional(keras.layers.LSTM(128, name=\"text_lstm\"), name=\"text_pool\")\n",
        "        text_embedding = lambda i: text_pool(word_embedding(i))\n",
        "        entity_desc_embedding = EntityFactsEmbedding(len(self._entity_encoder), len(self._predicate_encoder), dropout_layer)\n",
        "        from_seq = keras.layers.Reshape((embedding_size,), name=\"from_sequence\")\n",
        "        predicate_embedding = lambda i: from_seq(entity_desc_embedding._entity_predicates_embedding(i))\n",
        "        entity_embedding = lambda i: from_seq(entity_desc_embedding._entity_objects_embedding(i))\n",
        "\n",
        "        # constraint\n",
        "        constraint_id_input = keras.Input(shape=(1,), name=\"constraint_id\")\n",
        "        constraint_predicates_input = keras.Input(shape=(None,), name=\"constraint_predicates\")\n",
        "        constraint_objects_input = keras.Input(shape=(None,), name=\"constraint_objects\")\n",
        "        if with_constraint_id:\n",
        "            constraint_id_embedding = keras.layers.Embedding(len(self._constraint_id_encoder), embedding_size, name=\"constraint_id_embedding\")\n",
        "            constraint_features = from_seq(constraint_id_embedding(constraint_id_input))\n",
        "        else:\n",
        "            constraint_features = entity_desc_embedding(predicates=constraint_predicates_input, objects=constraint_objects_input)\n",
        "\n",
        "        # violation\n",
        "        subject_input = keras.Input(shape=(1,), name=\"subject\")\n",
        "        subject_predicates_input = keras.Input(shape=(None,), name=\"subject_predicates\")\n",
        "        subject_objects_input = keras.Input(shape=(None,), name=\"subject_objects\")\n",
        "        predicate_input = keras.Input(shape=(1,), name=\"predicate\")\n",
        "        object_input = keras.Input(shape=(1,), name=\"object\")\n",
        "        object_text_input = keras.Input(shape=(None,), name=\"object_text\")\n",
        "        object_predicates_input = keras.Input(shape=(None,), name=\"object_predicates\")\n",
        "        object_objects_input = keras.Input(shape=(None,), name=\"object_objects\")\n",
        "        other_subject_input = keras.Input(shape=(1,), name=\"other_subject\")\n",
        "        other_predicate_input = keras.Input(shape=(1,), name=\"other_predicate\")\n",
        "        other_object_input = keras.Input(shape=(1,), name=\"other_object\")\n",
        "        other_object_text_input = keras.Input(shape=(None,), name=\"other_object_text\")\n",
        "        other_entity_predicates_input = keras.Input(shape=(None,), name=\"other_entity_predicates\")\n",
        "        other_entity_objects_input = keras.Input(shape=(None,), name=\"other_entity_objects\")\n",
        "\n",
        "        subject_features = entity_embedding(subject_input)\n",
        "        predicate_features = predicate_embedding(predicate_input)\n",
        "        object_features = entity_embedding(object_input)\n",
        "        object_text_features = text_embedding(object_text_input)\n",
        "        other_subject_features = entity_embedding(other_subject_input)\n",
        "        other_predicate_features = predicate_embedding(other_predicate_input)\n",
        "        other_object_features = entity_embedding(other_object_input)\n",
        "        other_object_text_features = text_embedding(other_object_text_input)\n",
        "        if with_attention:\n",
        "            subject_query_features = keras.layers.Dense(units=embedding_size, name=\"subject_query_features\")(dropout_layer(constraint_features))\n",
        "            subject_desc_features = entity_desc_embedding(predicates=subject_predicates_input, objects=subject_objects_input, query=subject_query_features)\n",
        "        else:\n",
        "            subject_desc_features = entity_desc_embedding(predicates=subject_predicates_input, objects=subject_objects_input)\n",
        "        if with_attention:\n",
        "            object_query_features = keras.layers.Dense(units=embedding_size, name=\"object_query_features\")(dropout_layer(constraint_features))\n",
        "            object_desc_features = entity_desc_embedding(predicates=object_predicates_input, objects=object_objects_input, query=object_query_features)\n",
        "        else:\n",
        "            object_desc_features = entity_desc_embedding(predicates=object_predicates_input, objects=object_objects_input)\n",
        "        if with_attention:\n",
        "            other_entity_query_features = keras.layers.Dense(units=embedding_size, name=\"other_entity_query_features\")(dropout_layer(constraint_features))\n",
        "            other_entity_desc_features = entity_desc_embedding(predicates=other_entity_predicates_input, objects=other_entity_objects_input, query=other_entity_query_features)\n",
        "        else:\n",
        "            other_entity_desc_features = entity_desc_embedding(predicates=other_entity_predicates_input, objects=other_entity_objects_input)\n",
        "\n",
        "        inputs = [\n",
        "            constraint_features,\n",
        "            predicate_features,\n",
        "            object_features,\n",
        "            other_predicate_features,\n",
        "            other_object_features\n",
        "        ]\n",
        "        if with_subject:\n",
        "            inputs.append(subject_features)\n",
        "            inputs.append(other_subject_features)\n",
        "        if with_entity_facts:\n",
        "            inputs.append(subject_desc_features)\n",
        "            inputs.append(object_desc_features)\n",
        "            inputs.append(other_entity_desc_features)\n",
        "        if with_literals:\n",
        "            inputs.append(object_text_features)\n",
        "            inputs.append(other_object_text_features)\n",
        "        dense_input = dropout_layer(keras.layers.concatenate(inputs, name=\"input_concat\"))\n",
        "        dense_l1 = dropout_layer(keras.layers.Dense(units=embedding_size*4, activation='relu', name=\"dense_1\")(dense_input))\n",
        "        dense_l2 = dropout_layer(keras.layers.Dense(units=embedding_size*4, activation='relu', name=\"dense_2\")(dense_l1))\n",
        "        add_subject = keras.layers.Dense(units=len(self._output_entity_encoder), activation='softmax', name=\"add_subject\")(dense_l2)\n",
        "        add_predicate = keras.layers.Dense(units=len(self._output_predicate_encoder), activation='softmax', name=\"add_predicate\")(dense_l2)\n",
        "        add_object = keras.layers.Dense(units=len(self._output_entity_encoder), activation='softmax', name=\"add_object\")(dense_l2)\n",
        "        del_subject = keras.layers.Dense(units=len(self._output_entity_encoder), activation='softmax', name=\"del_subject\")(dense_l2)\n",
        "        del_predicate = keras.layers.Dense(units=len(self._output_predicate_encoder), activation='softmax', name=\"del_predicate\")(dense_l2)\n",
        "        del_object = keras.layers.Dense(units=len(self._output_entity_encoder), activation='softmax', name=\"del_object\")(dense_l2)\n",
        "\n",
        "        self._model = keras.Model(inputs=[\n",
        "            constraint_id_input, constraint_predicates_input,  constraint_objects_input,\n",
        "            subject_input,\n",
        "            subject_predicates_input, subject_objects_input,\n",
        "            predicate_input,\n",
        "            object_input, object_text_input,\n",
        "            object_predicates_input, object_objects_input,\n",
        "            other_subject_input,\n",
        "            other_predicate_input,\n",
        "            other_object_input, other_object_text_input,\n",
        "            other_entity_predicates_input, other_entity_objects_input\n",
        "        ], outputs=[add_subject, add_predicate, add_object, del_subject, del_predicate, del_object])\n",
        "        self._model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
        "        print('model build done')\n",
        "        print(self._model.summary())\n",
        "\n",
        "        print('training with {} epochs, a batch size of {} and a dropout rate of {}'.format(epochs, batch_size, dropout))\n",
        "\n",
        "        best_weights_filepath = './best_weights_corrections.keras'\n",
        "        early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, verbose=1, mode='auto')\n",
        "        save_best_model = keras.callbacks.ModelCheckpoint(best_weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "        history = self._model.fit(self._to_dataset(train_dataset, batch_size=batch_size, shuffle=True), epochs=epochs, validation_data=self._to_dataset(dev_dataset, batch_size=batch_size), callbacks=[early_stopping, save_best_model])\n",
        "        self._model.load_weights(best_weights_filepath)\n",
        "\n",
        "    def _to_dataset(self, dataset: Dict[str,np.array], batch_size, shuffle: bool = False):\n",
        "        return DatasetSequence(dataset, self._constraint_id_encoder, self._predicate_encoder, self._entity_encoder, self._output_predicate_encoder, self._output_entity_encoder, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "    def eval(self, dataset: Dict[str,np.array]):\n",
        "        ok_by_constraint = defaultdict(int)\n",
        "        error_by_constraint = defaultdict(int)\n",
        "        total_by_constraint = defaultdict(int)\n",
        "        ok = 0\n",
        "        error = 0\n",
        "        total = 0\n",
        "        parameters_found_and_correct = defaultdict(int)\n",
        "        parameters_predicted = defaultdict(int)\n",
        "        parameters_expected = defaultdict(int)\n",
        "\n",
        "        for add_subject_pred, add_predicate_pred, add_object_pred, del_subject_pred, del_predicate_pred, del_object_pred in model.predict(dataset):\n",
        "            constraint = dataset['constraint_id'][total]\n",
        "            predictions = {\n",
        "                'add_subject': add_subject_pred,\n",
        "                'add_predicate': add_predicate_pred,\n",
        "                'add_object': add_object_pred,\n",
        "                'del_subject': del_subject_pred,\n",
        "                'del_predicate': del_predicate_pred,\n",
        "                'del_object': del_object_pred,\n",
        "            }\n",
        "            if predictions['add_subject'] != 0 or predictions['add_predicate'] != 0 or predictions['add_object'] != 0:\n",
        "                if predictions['add_subject'] == 0 or predictions['add_predicate'] == 0 or predictions['add_object'] == 0:\n",
        "                    total += 1\n",
        "                    total_by_constraint[constraint] += 1\n",
        "                    continue # Missing value\n",
        "            if predictions['del_subject'] != 0 or predictions['del_predicate'] != 0 or predictions['del_object'] != 0:\n",
        "                if predictions['del_subject'] == 0 or predictions['del_predicate'] == 0 or predictions['del_object'] == 0:\n",
        "                    total += 1\n",
        "                    total_by_constraint[constraint] += 1\n",
        "                    continue # Missing value\n",
        "\n",
        "            if all(dataset[k][total] == v for k, v in predictions.items()):\n",
        "                ok += 1\n",
        "                ok_by_constraint[constraint] += 1\n",
        "            else:\n",
        "                error += 1\n",
        "                error_by_constraint[constraint] += 1\n",
        "            for k, v in predictions.items():\n",
        "                if v is not None:\n",
        "                    parameters_predicted[k] += 1\n",
        "                    if v == dataset[k][total] :\n",
        "                        parameters_found_and_correct[k] += 1\n",
        "                if dataset[k][total] is not None:\n",
        "                    parameters_expected[k] += 1\n",
        "            total_by_constraint[constraint] += 1\n",
        "            total += 1\n",
        "\n",
        "        by_constraint = [self._precision_recall(ok_by_constraint[c], ok_by_constraint[c] + error_by_constraint[c], total_by_constraint[c]) for c in total_by_constraint.keys()]\n",
        "        return {\n",
        "            **self._precision_recall(ok, ok+error, total),\n",
        "            'accuracy': ok/total,\n",
        "            'parameters': {k: self._precision_recall(parameters_found_and_correct[k], parameters_predicted[k],v) for k,v in parameters_expected.items()},\n",
        "            'by_constraint': by_constraint,\n",
        "            'ok': ok,\n",
        "            'error': error,\n",
        "            'total': total\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def _precision_recall(found_and_correct: int, predicted: int, expected: int) -> dict:\n",
        "        precision = found_and_correct / predicted if predicted else float('nan')\n",
        "        recall = found_and_correct / expected if expected else float('nan') # TODO: should be found and correct ??? This seems badly wrong and it's what we did in the CorHist paper\n",
        "        F1 = 2 * precision*recall / (precision+recall) if precision + recall else float('nan')\n",
        "        return {\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'F1': F1\n",
        "        }\n",
        "\n",
        "    def predict(self, dataset: Dict[str,np.array]):\n",
        "        dataset = self._to_dataset(dataset, batch_size=128)\n",
        "        for i in range(len(dataset)):\n",
        "            add_subject_pred, add_predicate_pred, add_object_pred, del_subject_pred, del_predicate_pred, del_object_pred = self._model.predict(dataset[i][0])\n",
        "            add_subject_pred = self._output_entity_encoder.decode(np.argmax(add_subject_pred, 1))\n",
        "            add_predicate_pred = self._output_predicate_encoder.decode(np.argmax(add_predicate_pred, 1))\n",
        "            add_object_pred = self._output_entity_encoder.decode(np.argmax(add_object_pred, 1))\n",
        "            del_subject_pred = self._output_entity_encoder.decode(np.argmax(del_subject_pred, 1))\n",
        "            del_predicate_pred = self._output_predicate_encoder.decode(np.argmax(del_predicate_pred, 1))\n",
        "            del_object_pred = self._output_entity_encoder.decode(np.argmax(del_object_pred, 1))\n",
        "            yield from zip(add_subject_pred, add_predicate_pred, add_object_pred, del_subject_pred, del_predicate_pred, del_object_pred)\n",
        "\n",
        "    def save(self, dir: str):\n",
        "        os.makedirs(dir, exist_ok=True)\n",
        "        self._entity_encoder.save(dir + '/entity_encoding.txt')\n",
        "        self._output_entity_encoder.save(dir + '/output_entity_encoding.txt')\n",
        "        self._predicate_encoder.save(dir + '/predicate_encoding.txt')\n",
        "        self._output_predicate_encoder.save(dir + '/output_predicate_encoding.txt')\n",
        "        self._constraint_id_encoder.save(dir + '/constraint_id_encoding.txt')\n",
        "        self._model.save(dir + '/model')"
      ],
      "metadata": {
        "id": "EuaGoSonPD1w"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = None\n",
        "# hack to clear memory before creating the new model\n",
        "gc.collect()\n",
        "model = Model(train_dataset, dev_dataset, epochs=20, batch_size=256, dropout=0.1,\n",
        "              with_attention=False, with_entity_facts=True, with_literals=True, with_constraint_id=False, with_subject=False)\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "TI4emq__PoEI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "03dcfae6-b833-41ff-8988-7fb928b9e58f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset stats: 238 input predicates, 330 input entities, 6 output predicates, 8 output entities\n",
            "model build done\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " constraint_predica  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " constraint_objects   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " predicate            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " object (\u001b[38;5;33mInputLayer\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  -                 \n",
              "\n",
              " other_predicate      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " other_object         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " subject_predicates   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " subject_objects      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " object_predicates    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " object_objects       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " other_entity_predi  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " other_entity_objec  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " predicate_embedding  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      \u001b[38;5;34m30,464\u001b[0m  constraint_predi \n",
              " (\u001b[38;5;33mEmbedding\u001b[0m)                                         predicate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  \n",
              "                                                     other_predicate[\u001b[38;5;34m\u001b[0m \n",
              "                                                     subject_predicat \n",
              "                                                     object_predicate \n",
              "                                                     other_entity_pre \n",
              "\n",
              " entity_embedding     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      \u001b[38;5;34m42,240\u001b[0m  constraint_objec \n",
              " (\u001b[38;5;33mEmbedding\u001b[0m)                                         object[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     other_object[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              "                                                     subject_objects[\u001b[38;5;34m\u001b[0m \n",
              "                                                     object_objects[\u001b[38;5;34m0\u001b[0m \n",
              "                                                     other_entity_obj \n",
              "\n",
              " entity_desc_combin  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           \u001b[38;5;34m0\u001b[0m  predicate_embedd \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                       entity_embedding \n",
              "                                                     predicate_embedd \n",
              "                                                     entity_embedding \n",
              "                                                     predicate_embedd \n",
              "                                                     entity_embedding \n",
              "                                                     predicate_embedd \n",
              "                                                     entity_embedding \n",
              "\n",
              " dropout (\u001b[38;5;33mDropout\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  entity_desc_comb \n",
              "                                                     entity_desc_comb \n",
              "                                                     entity_desc_comb \n",
              "                                                     entity_desc_comb \n",
              "                                                     input_concat[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              "                                                     dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    \n",
              "                                                     dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " object_text          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " other_object_text    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " entity_desc_featur  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      \u001b[38;5;34m32,896\u001b[0m  dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    \n",
              " (\u001b[38;5;33mDense\u001b[0m)                                             dropout[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m],    \n",
              "                                                     dropout[\u001b[38;5;34m2\u001b[0m][\u001b[38;5;34m0\u001b[0m],    \n",
              "                                                     dropout[\u001b[38;5;34m3\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " text_embedding       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)   \u001b[38;5;34m3,840,000\u001b[0m  object_text[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mEmbedding\u001b[0m)                                         other_object_tex \n",
              "\n",
              " entity_desc_reduct  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  entity_desc_feat \n",
              " (\u001b[38;5;33mGlobalMaxPooling1\u001b[0m                                 entity_desc_feat \n",
              "                                                     entity_desc_feat \n",
              "                                                     entity_desc_feat \n",
              "\n",
              " from_sequence        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  predicate_embedd \n",
              " (\u001b[38;5;33mReshape\u001b[0m)                                           entity_embedding \n",
              "                                                     predicate_embedd \n",
              "                                                     entity_embedding \n",
              "\n",
              " text_pool            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  text_embedding[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mGlobalMaxPooling1\u001b[0m                                 text_embedding[\u001b[38;5;34m1\u001b[0m \n",
              "\n",
              " input_concat         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                \u001b[38;5;34m0\u001b[0m  entity_desc_redu \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                       from_sequence[\u001b[38;5;34m1\u001b[0m] \n",
              "                                                     from_sequence[\u001b[38;5;34m2\u001b[0m] \n",
              "                                                     from_sequence[\u001b[38;5;34m4\u001b[0m] \n",
              "                                                     from_sequence[\u001b[38;5;34m5\u001b[0m] \n",
              "                                                     entity_desc_redu \n",
              "                                                     entity_desc_redu \n",
              "                                                     entity_desc_redu \n",
              "                                                     text_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  \n",
              "                                                     text_pool[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " dense_1 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)           \u001b[38;5;34m655,872\u001b[0m  dropout[\u001b[38;5;34m4\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " dense_2 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)           \u001b[38;5;34m262,656\u001b[0m  dropout[\u001b[38;5;34m5\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " constraint_id        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " subject              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " other_subject        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " add_subject (\u001b[38;5;33mDense\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)               \u001b[38;5;34m4,104\u001b[0m  dropout[\u001b[38;5;34m6\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " add_predicate        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)               \u001b[38;5;34m3,078\u001b[0m  dropout[\u001b[38;5;34m6\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              " (\u001b[38;5;33mDense\u001b[0m)                                                               \n",
              "\n",
              " add_object (\u001b[38;5;33mDense\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)               \u001b[38;5;34m4,104\u001b[0m  dropout[\u001b[38;5;34m6\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " del_subject (\u001b[38;5;33mDense\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)               \u001b[38;5;34m4,104\u001b[0m  dropout[\u001b[38;5;34m6\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " del_predicate        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)               \u001b[38;5;34m3,078\u001b[0m  dropout[\u001b[38;5;34m6\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              " (\u001b[38;5;33mDense\u001b[0m)                                                               \n",
              "\n",
              " del_object (\u001b[38;5;33mDense\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)               \u001b[38;5;34m4,104\u001b[0m  dropout[\u001b[38;5;34m6\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
              "\n",
              " constraint_predica  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " constraint_objects   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " predicate            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " object (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              "\n",
              " other_predicate      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " other_object         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " subject_predicates   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " subject_objects      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " object_predicates    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " object_objects       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " other_entity_predi  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " other_entity_objec  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " predicate_embedding  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">30,464</span>  constraint_predi \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                                         predicate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  \n",
              "                                                     other_predicate[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              "                                                     subject_predicat \n",
              "                                                     object_predicate \n",
              "                                                     other_entity_pre \n",
              "\n",
              " entity_embedding     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">42,240</span>  constraint_objec \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                                         object[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     other_object[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              "                                                     subject_objects[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              "                                                     object_objects[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "                                                     other_entity_obj \n",
              "\n",
              " entity_desc_combin  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  predicate_embedd \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                       entity_embedding \n",
              "                                                     predicate_embedd \n",
              "                                                     entity_embedding \n",
              "                                                     predicate_embedd \n",
              "                                                     entity_embedding \n",
              "                                                     predicate_embedd \n",
              "                                                     entity_embedding \n",
              "\n",
              " dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  entity_desc_comb \n",
              "                                                     entity_desc_comb \n",
              "                                                     entity_desc_comb \n",
              "                                                     entity_desc_comb \n",
              "                                                     input_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              "                                                     dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    \n",
              "                                                     dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " object_text          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " other_object_text    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " entity_desc_featur  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span>  dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                             dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    \n",
              "                                                     dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    \n",
              "                                                     dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " text_embedding       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   <span style=\"color: #00af00; text-decoration-color: #00af00\">3,840,000</span>  object_text[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                                         other_object_tex \n",
              "\n",
              " entity_desc_reduct  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  entity_desc_feat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1</span>                                 entity_desc_feat \n",
              "                                                     entity_desc_feat \n",
              "                                                     entity_desc_feat \n",
              "\n",
              " from_sequence        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  predicate_embedd \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)                                           entity_embedding \n",
              "                                                     predicate_embedd \n",
              "                                                     entity_embedding \n",
              "\n",
              " text_pool            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  text_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1</span>                                 text_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span> \n",
              "\n",
              " input_concat         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  entity_desc_redu \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                       from_sequence[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>] \n",
              "                                                     from_sequence[<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>] \n",
              "                                                     from_sequence[<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>] \n",
              "                                                     from_sequence[<span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>] \n",
              "                                                     entity_desc_redu \n",
              "                                                     entity_desc_redu \n",
              "                                                     entity_desc_redu \n",
              "                                                     text_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  \n",
              "                                                     text_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">655,872</span>  dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span>  dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " constraint_id        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " subject              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " other_subject        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " add_subject (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,104</span>  dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " add_predicate        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">3,078</span>  dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                                               \n",
              "\n",
              " add_object (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,104</span>  dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " del_subject (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,104</span>  dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " del_predicate        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">3,078</span>  dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                                               \n",
              "\n",
              " del_object (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,104</span>  dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,886,700\u001b[0m (18.64 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,886,700</span> (18.64 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,886,700\u001b[0m (18.64 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,886,700</span> (18.64 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "training with 20 epochs, a batch size of 256 and a dropout rate of 0.1\n",
            "shuffling dataset\n",
            "Indices: [2446 1019 2129 3822 1168 2754 2249 4401  866 3334 2160 2929 3005 2341\n",
            " 1417 4139 4160 1533 3897  220 3021 4174 4295 3187  453   96 1710  533\n",
            " 4110 1364 2006  896  991 4498  753 3088  474 3595 2779 1205 1897 1891\n",
            "  525 2246 1938  795 4116 4196 3043 4007 1119  404 2081 2050  711 3602\n",
            " 3415 1173 1402 1460 2128  301 4330   41 4338 1662 4372 3520  971 1287\n",
            " 1563 1440 3920 1032 3410  765 2758  396 1286  329 3186 3381 2313 1656\n",
            "  542 3391 4419 1077 3740 2883 2481  240 3793  512 1060 2922  651 1876\n",
            " 4215  541 1340 2672 4314 1846 1484 2232 2790 3367 2846 1112 1040 3609\n",
            " 3877 4362 2838 2409 1596 2970 2679 1705  478 3659  493 1885 3805 2323\n",
            " 1886 3518 4415  235 2507 4038 1799 3741  881 3825  204 2146 3790 2334\n",
            "   26 2184 2868  143 1282 1715  199  728  946 3763  446 3516 3127 3663\n",
            "  975 2887 1436  151 3279 3158 3539 1039  298 2188 2338 1450  771 1661\n",
            " 2984 1022 4089  331 2432 3513 1155 3575 1355 1031 2046 2197  637 3174\n",
            " 1088 1771  283 2469 4386 4460  254 2288  760 1301 2828 2414 1473 3250\n",
            " 4032   85 3842  188 2343 1366  495   80 3621  677 1349 1843  266 1483\n",
            "  120 3692 3744 1399  693  548 3034 3374  181 2741 3611 4061 1029 1538\n",
            " 1942  128 1960 1059 2296 4479 4383 2477 3129 2182 4167 1321 3246 2358\n",
            " 4296 1050   11  724 2827 4381 1674  275  411 2542 3814 4321 4409 1633\n",
            " 3989 1246 4345 2123]\n",
            "Batch keys: dict_keys(['constraint_id', 'constraint_predicates', 'constraint_objects', 'subject', 'subject_predicates', 'subject_objects', 'predicate', 'object', 'object_text', 'object_predicates', 'object_objects', 'other_subject', 'other_predicate', 'other_object', 'other_object_text', 'other_entity_predicates', 'other_entity_objects', 'add_subject', 'add_predicate', 'add_object', 'del_subject', 'del_predicate', 'del_object'])\n",
            "constraint_id: shape=(256,)\n",
            "constraint_predicates: shape=(256,)\n",
            "constraint_objects: shape=(256,)\n",
            "subject: shape=(256,)\n",
            "subject_predicates: shape=(256,)\n",
            "subject_objects: shape=(256,)\n",
            "predicate: shape=(256,)\n",
            "object: shape=(256,)\n",
            "object_text: shape=(256,)\n",
            "object_predicates: shape=(256,)\n",
            "object_objects: shape=(256,)\n",
            "other_subject: shape=(256,)\n",
            "other_predicate: shape=(256,)\n",
            "other_object: shape=(256,)\n",
            "other_object_text: shape=(256,)\n",
            "other_entity_predicates: shape=(256,)\n",
            "other_entity_objects: shape=(256,)\n",
            "add_subject: shape=(256,)\n",
            "add_predicate: shape=(256,)\n",
            "add_object: shape=(256,)\n",
            "del_subject: shape=(256,)\n",
            "del_predicate: shape=(256,)\n",
            "del_object: shape=(256,)\n",
            "Indices: [1579  170 3946 3560 3576 3914 2657 1998 3338 1691 1053 2007 1944   64\n",
            " 1686 3945 3811 3126 1593 2248 4440  995 1896 1893 2322 3276 2207 3449\n",
            " 2185 2598 1904 1996 2351  859 2095 2179 3622 4229  234 3368 2732 2033\n",
            "  932 3270 4144 1775 2499 3801 2028  972 3315  434 3216 4228  308 3869\n",
            " 3701 3474  895   71  431  317 2663  576  247 1176 3808  834 2806 3931\n",
            " 4413 1442  524 2622 4243 4266  856 1732 1362  342 1625 4404 1438  354\n",
            " 1664 1928 2444 4436 2178 2944 3451 1142 3690 4374 1845  250 3508 2879\n",
            " 1833 4102 2244 2927 1446 2272 2864  169 2032 2619 3128    7  689 1281\n",
            " 2803 1472 3774  374 3053 1407 4326 3092 2009 1786 2433 3437 4192 2386\n",
            "  355 3850  870 1706  371 3846  194 1930 3994 1887 4394 2824  500 1072\n",
            " 1434 3795  574 2346 2775 1527 1135 2686  141 1881   32 2704 1720 2588\n",
            "  370 2215 4280 2101 3786 2858 2077 3382 4261  365 1400 4292 2674 4094\n",
            "  764 1818 3306  241 3453 4046 1359 3287 1268 3219 1524 1779 2968 3448\n",
            " 2885 4050 3067 3610 3156 1233 1284 1685 3901 2500  674 1642 2904 1285\n",
            " 4287 4388 3594 1687 4435 3159 4483 2987 4494 4076  621 2110 2079 3909\n",
            " 1302 1352 1030 1865 4256 1767 3681 3352 3031 1585 4491  943 3266 2353\n",
            " 2200 1966 3685  424 3197 3921 4391 1095  223 1383 1429  196 3626 4111\n",
            " 1526 1189 4193 1850  589  117 3396 1116 3873  243 2914  177 3309 3091\n",
            " 3213 3872  833 3883]\n",
            "Batch keys: dict_keys(['constraint_id', 'constraint_predicates', 'constraint_objects', 'subject', 'subject_predicates', 'subject_objects', 'predicate', 'object', 'object_text', 'object_predicates', 'object_objects', 'other_subject', 'other_predicate', 'other_object', 'other_object_text', 'other_entity_predicates', 'other_entity_objects', 'add_subject', 'add_predicate', 'add_object', 'del_subject', 'del_predicate', 'del_object'])\n",
            "constraint_id: shape=(256,)\n",
            "constraint_predicates: shape=(256,)\n",
            "constraint_objects: shape=(256,)\n",
            "subject: shape=(256,)\n",
            "subject_predicates: shape=(256,)\n",
            "subject_objects: shape=(256,)\n",
            "predicate: shape=(256,)\n",
            "object: shape=(256,)\n",
            "object_text: shape=(256,)\n",
            "object_predicates: shape=(256,)\n",
            "object_objects: shape=(256,)\n",
            "other_subject: shape=(256,)\n",
            "other_predicate: shape=(256,)\n",
            "other_object: shape=(256,)\n",
            "other_object_text: shape=(256,)\n",
            "other_entity_predicates: shape=(256,)\n",
            "other_entity_objects: shape=(256,)\n",
            "add_subject: shape=(256,)\n",
            "add_predicate: shape=(256,)\n",
            "add_object: shape=(256,)\n",
            "del_subject: shape=(256,)\n",
            "del_predicate: shape=(256,)\n",
            "del_object: shape=(256,)\n",
            "shuffling dataset\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "For a model with multiple outputs, when providing the `metrics` argument as a list, it should have as many entries as the model has outputs. Received:\nmetrics=['sparse_categorical_accuracy']\nof length 1 whereas the model has 6 outputs.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-19-1453437486.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# hack to clear memory before creating the new model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m model = Model(train_dataset, dev_dataset, epochs=20, batch_size=256, dropout=0.1,\n\u001b[0m\u001b[1;32m      5\u001b[0m               with_attention=False, with_entity_facts=True, with_literals=True, with_constraint_id=False, with_subject=False)\n\u001b[1;32m      6\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-18-452147233.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_dataset, dev_dataset, epochs, batch_size, dropout, with_attention, with_entity_facts, with_literals, with_constraint_id, with_subject)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0msave_best_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_weights_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_model\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_weights_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/trainers/compile_utils.py\u001b[0m in \u001b[0;36m_build_metrics_set\u001b[0;34m(self, metrics, num_outputs, output_names, y_true, y_pred, argument_name)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    255\u001b[0m                         \u001b[0;34m\"For a model with multiple outputs, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                         \u001b[0;34mf\"when providing the `{argument_name}` argument as a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: For a model with multiple outputs, when providing the `metrics` argument as a list, it should have as many entries as the model has outputs. Received:\nmetrics=['sparse_categorical_accuracy']\nof length 1 whereas the model has 6 outputs."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sT_Wn2VJnb5l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}